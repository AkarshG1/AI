{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "import random \n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor policy_evaluation, policy_improvement, policy_iteration and value_iteration,\\nthe parameters P, nS, nA, gamma are defined as follows:\\n\\n\\tP: nested dictionary\\n\\t\\tFrom gym.core.Environment\\n\\t\\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\\n\\t\\ttuple of the form (probability, nextstate, reward, terminal) where\\n\\t\\t\\t- probability: float\\n\\t\\t\\t\\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\\n\\t\\t\\t- nextstate: int\\n\\t\\t\\t\\tdenotes the state we transition to (in range [0, nS - 1])\\n\\t\\t\\t- reward: int\\n\\t\\t\\t\\teither 0 or 1, the reward for transitioning from \"state\" to\\n\\t\\t\\t\\t\"nextstate\" with \"action\"\\n\\t\\t\\t- terminal: bool\\n\\t\\t\\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\\n\\tnS: int\\n\\t\\tnumber of states in the environment\\n\\tnA: int\\n\\t\\tnumber of actions in the environment\\n\\tgamma: float\\n\\t\\tDiscount factor. Number in range [0, 1)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "$V^{\\pi}_{k}(s) \\leftarrow \\underset{a}{\\sum}\\pi(a|s) \\underset{s'}{\\sum} P(s'|s,\\pi (s))(R(s,\\pi(s),s')+ \\gamma V^{\\pi}_{k-1}(s'))$\n",
    "\n",
    "## Policy Improvement\n",
    "\n",
    "$\\pi_{k+1}(s) \\leftarrow \\underset{a}{argmax}\\ q^{\\pi}(s,a)$\n",
    "\n",
    "Where $ q^{\\pi}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)(R(s,\\pi(s),s')+ \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "1. Initialize Policy $\\pi$\n",
    "\n",
    "2. Evaluate: Until $V^{\\pi}_{k}(s) - V^{\\pi}_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , evalute $\\pi$ as $V^{\\pi}_{k}(s)$\n",
    "3. Improve: If $\\pi_{k}(s) \\ \\ != \\pi_{k+1}(s) <$ perform policy improvement using $V^{\\pi}_{k}(s)$\n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "1. Initialize $V(s)$ for all s in $\\mathbb S $\n",
    "2. Until $V_{k}(s) - V_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , $V_{k}(s)$ as\n",
    "\n",
    "$V_{k}(s) = \\underset{a}{max} \\underset{s}{\\sum} P(s'|s,a)(R(s,a,s')+ \\gamma V(s'))$\n",
    "3. Perform policy improvement using $V_{k}(s)$ such that $ \\pi \\approx \\pi *$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isover(V,V_new,tol):\n",
    "\tif np.all(np.abs(V - V_new) < tol) :    #np.sum(np.sqrt(np.square(V_new-V))) < tol\n",
    "\t\treturn 1\n",
    "\treturn 0\n",
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9,  tol=1e-3):\n",
    "\t\"\"\"Evaluate the value function from a given policy.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP: dictionary\n",
    "\t\tIt is from gym.core.Environment\n",
    "\t\tP[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "\tnS: int\n",
    "\t\tnumber of states\n",
    "\tnA: int\n",
    "\t\tnumber of actions\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\tpolicy: np.array\n",
    "\t\tThe policy to evaluate. Maps states to actions.\n",
    "\tmax_iteration: int\n",
    "\t\tThe maximum number of iterations to run before stopping. Feel free to change it.\n",
    "\ttol: float\n",
    "\t\tDetermines when value function has converged.\n",
    "\tReturns\n",
    "\t-------\n",
    "\tvalue function: np.ndarray\n",
    "\t\tThe value function from the given policy.\n",
    "\t\"\"\"\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\t############################\n",
    "\tV=np.zeros(nS)\n",
    "\tV_new=V.copy()\n",
    "\ti=0\n",
    "\tfor i in range(max_iteration):\n",
    "\t\tV=V_new.copy()\n",
    "\t\tV_new = np.zeros(nS, dtype=float)\n",
    "\t\tfor state in range(nS):\n",
    "\t\t\tfor probability, nextstate, reward, terminal in P[state][policy[state]]:\n",
    "\t\t\t\tV_new[state] += probability * (reward + gamma * V[nextstate])\n",
    "\t\tif isover(V,V_new,tol) :\n",
    "\t\t\tbreak\n",
    "\treturn V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"Given the value function from policy improve the policy.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP: dictionary\n",
    "\t\tIt is from gym.core.Environment\n",
    "\t\tP[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "\tnS: int\n",
    "\t\tnumber of states\n",
    "\tnA: int\n",
    "\t\tnumber of actions\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew policy: np.ndarray\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\t############################\n",
    "\tP_new = np.zeros(nS, dtype=int)\n",
    "\tfor state in range(nS):\n",
    "\t\tB=np.zeros(nA,dtype=float)\n",
    "\t\tq=-99\n",
    "\t\tfor action in range(nA):\n",
    "\t\t\tfor probability, nextstate, reward, terminal in P[state][action]:\n",
    "\t\t\t\tB[action] += probability * (reward + gamma * value_from_policy[nextstate])\n",
    "\t\t\tif(B[action]>q):\n",
    "\t\t\t\tq=B[action]\n",
    "\t\t\t\tP_new[state]=action\n",
    "\t\t\telif(q == B[action]):\n",
    "\t\t\t\tif random.random() < 0.5:\n",
    "\t\t\t\t\tP_new[state]=action\n",
    "\treturn P_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "\t\"\"\"Runs policy iteration.\n",
    "\tYou should use the policy_evaluation and policy_improvement methods to\n",
    "\timplement this method.\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP: dictionary\n",
    "\t\tIt is from gym.core.Environment\n",
    "\t\tP[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "\tnS: int\n",
    "\t\tnumber of states\n",
    "\tnA: int\n",
    "\t\tnumber of actions\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\tmax_iteration: int\n",
    "\t\tThe maximum number of iterations to run before stopping. Feel free to change it.\n",
    "\ttol: float\n",
    "\t\tDetermines when value function has converged.\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue function: np.ndarray\n",
    "\tpolicy: np.ndarray\n",
    "\t\"\"\"\n",
    "\tV = np.zeros(nS,dtype=float)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\tfor s in range(nS):\n",
    "\t\tpolicy[s]=s%nA\n",
    "\tfor i in range(max_iteration):\n",
    "\t\tV_new=policy_evaluation(P, nS, nA, policy, gamma)\n",
    "\t\tpolicy_new=policy_improvement(P, nS, nA, V_new, policy, gamma)\n",
    "\t\tif isover(V,V_new,tol) :\n",
    "\t\t\tbreak\n",
    "\t\tV=V_new.copy()\n",
    "\t\tpolicy=policy_new.copy()\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\t############################\n",
    "\treturn V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, max_iteration=20, tol=1e-3):\n",
    "\t\"\"\"\n",
    "\tLearn value function and policy by using value iteration method for a given\n",
    "\tgamma and environment.\n",
    "\tParameters:\n",
    "\t----------\n",
    "\tP: dictionary\n",
    "\t\tIt is from gym.core.Environment\n",
    "\t\tP[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "\tnS: int\n",
    "\t\tnumber of states\n",
    "\tnA: int\n",
    "\t\tnumber of actions\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "\tmax_iteration: int\n",
    "\t\tThe maximum number of iterations to run before stopping. Feel free to change it.\n",
    "\ttol: float\n",
    "\t\tDetermines when value function has converged.\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue function: np.ndarray\n",
    "\tpolicy: np.ndarray\n",
    "\t\"\"\"\n",
    "\tV = np.zeros(nS,dtype=float)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\tfor i in range(max_iteration):\n",
    "\t\tV_next=np.zeros(nS,dtype=float)\n",
    "\t\tfor s in range(nS):\n",
    "\t\t\tfor a in range(nA):\n",
    "\t\t\t\tq=0\n",
    "\t\t\t\tfor probability, nextstate, reward, terminal in P[s][a]:\n",
    "\t\t\t\t\tq += probability * (reward + gamma * V[nextstate])\n",
    "\t\t\t\tif V_next[s] < q:\n",
    "\t\t\t\t\tV_next[s] = q\n",
    "\t\tif isover(V,V_next,tol):\n",
    "\t\t\tbreak\n",
    "\t\tV = V_next.copy()\n",
    "\tpolicy=policy_improvement(P, nS, nA, V_next, policy, gamma)\n",
    "\t############################\n",
    "\t# YOUR IMPLEMENTATION HERE #\n",
    "\t############################\n",
    "\treturn V_next, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "The agent didn't reach a terminal state in 100 steps.\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "#env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env, p_pi, 100)\n",
    "\n",
    "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "render_single(env, p_vi, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
