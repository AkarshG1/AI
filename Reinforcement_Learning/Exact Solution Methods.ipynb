{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandits\n",
    "\n",
    "Choose from K different actions repeatedly and receive a numerical reward chosen from a stationary probability distribution conditional on the selected action each time independent of the previous action(s). \n",
    "\n",
    "In the K-armed Bandit problem, each action $a$ has an expected reward $q_*(a)$. This is called the value of that action.\n",
    "\n",
    "$  q_*(a) \\doteq \\mathbb E[R_t | A_t=a] $\n",
    "\n",
    "Knowing the value of each action would enable us to solve the problem by simply selecting the highest-valued action always. However, it is likely we will have only estimates of the action values at any time, $Q_t(a)$. \n",
    "The objective is to get $Q_t(a)$ as close to $q_t(a)$ as possible.\n",
    "\n",
    "### Action-value Methods\n",
    "\n",
    "One way to estimate $Q_t(a)$ is to average rewards over the timesteps\n",
    "\n",
    "$Q_t(a) \\doteq \\frac{\\sum^{t-1}_{i=1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum^{t-1}_{i=1}\\mathbb 1_{A_i=a}}$\n",
    "\n",
    "The best way to select an action, a.k.a the policy would be 'greedy':\n",
    "\n",
    "$A_t \\doteq \\underset{a}{argmax} Q_t(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value Function\n",
    "$V^{\\pi}(s) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{\\pi}(s')) $ \n",
    "\n",
    "## Action Value Function\n",
    "$Q^{\\pi}(s) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{\\pi}(s')) $ \n",
    "\n",
    "## Optimal Value Function\n",
    "$V^{*}(s) = \\underset{a}{max} \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{*}(s')) $ \n",
    "\n",
    "## Optimal Action Value Function\n",
    "$Q^{*}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma \\cdot \\underset{a}{max} \\ Q^{*}(s',a')) $ \n",
    "\n",
    "k is timesteps from horizon\n",
    "\n",
    "## Dynamic Programming \n",
    "\n",
    "The term dynamic programming (DP) refers to a collection of algorithms that can be\n",
    "used to compute optimal policies given a perfect model of the environment as a Markov\n",
    "decision process (MDP).\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "$V^{\\pi}_{k}(s) \\leftarrow \\underset{a}{\\sum}\\pi(a|s) \\underset{s'}{\\sum} P(s'|s,\\pi (s))(R(s,\\pi(s),s')+ \\gamma V^{\\pi}_{k-1}(s'))$\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "$\\pi_{k+1}(s) \\leftarrow \\underset{a}{argmax}\\ q^{\\pi}(s,a)$\n",
    "\n",
    "Where $ q^{\\pi}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)(R(s,\\pi(s),s')+ \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "1. Initialize Policy $\\pi$\n",
    "\n",
    "2. Evaluate: Until $V^{\\pi}_{k}(s) - V^{\\pi}_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , evalute $\\pi$ as $V^{\\pi}_{k}(s)$\n",
    "3. Improve: If $\\pi_{k}(s) \\ \\ != \\pi_{k+1}(s) <$ perform policy improvement using $V^{\\pi}_{k}(s)$\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "1. Initialize $V(s)$ for all s in $\\mathbb S $\n",
    "2. Until $V_{k}(s) - V_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , $V_{k}(s)$ as\n",
    "\n",
    "$V_{k}(s) = \\underset{a}{max} \\underset{s}{\\sum} P(s'|s,a)(R(s,a,s')+ \\gamma V(s'))$\n",
    "3. Perform policy improvement using $V_{k}(s)$ such that $ \\pi \\approx \\pi *$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged(V,V_new, tol=10e-3):\n",
    "    \"\"\"\n",
    "    Checks whether first two arguments have converged to within the third argument, tolerance.\n",
    "    Arguments:\n",
    "        V : Numpy float array\n",
    "        V_new: Numpy float array\n",
    "        tol: float\n",
    "    Returns:\n",
    "        bool True or False\n",
    "    \"\"\"\n",
    "    if np.all(np.abs(V - V_new) < tol) : \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, max_episodes=10):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        state,reward,done,info = env.step(policy[state])\n",
    "        print(state,reward,done,info)\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, P, nS, nA, gamma=0.9, max_iter=1000, tol=10e-3):\n",
    "    V = np.zeros(nS)\n",
    "    V_new = V.copy()\n",
    "    for i in range(max_iter):\n",
    "        V = V_new.copy()\n",
    "        V_new = np.zeros(nS)\n",
    "        for state in range(nS):\n",
    "            for probability, next_state, reward, done in P[state][pi[state]]: \n",
    "                V_new[state] += probability*(reward + gamma*V[next_state])\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(pi, P, V, nS, nA, gamma=0.9):\n",
    "    pi_new = np.zeros(nS, dtype='int')\n",
    "    for state in range(nS):\n",
    "        B = np.zeros(nA)\n",
    "        q = -99\n",
    "        for action in range(nA):\n",
    "            for probability, next_state, reward, done in P[state][action]: \n",
    "                B[action] += probability*(reward + gamma*V[next_state])\n",
    "            if B[action]>q:\n",
    "                q = B[action]\n",
    "                pi_new[state] = action\n",
    "            elif B[action] == q:\n",
    "                if np.random.random() < 0.5:\n",
    "                    pi_new[state] == action       \n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=1000, gamma=0.9, tol=10e-3):  \n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = np.zeros(nS, dtype=float)\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                q = 0\n",
    "                for probability, next_state, reward, done in P[state][action]: \n",
    "                    q += probability*(reward + gamma*V[next_state])\n",
    "                if V_new[state] < q:\n",
    "                    V_new[state] = q\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        V = V_new.copy()\n",
    "    pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iter=20, gamma=0.9, tol=10e-3):\n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = evaluate_policy(pi,P,nS,nA,gamma=gamma,tol=tol)\n",
    "        pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "        \n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        \n",
    "        V = V_new.copy()\n",
    "        pi=pi_new.copy()\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 2]]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_A_E  = [[1,0,2],[2,1,3],[3,4,5]]\n",
    "S_A_E[:2][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[[]]*3]*4\n",
    "a[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(env, max_iter = 10, gamma=0.99):\n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    \n",
    "    # initialize Q abitrarily for all states and actions\n",
    "    Q = np.zeros([nS,nA], dtype=float)\n",
    "    # initialize Returns as an empty list\n",
    "    Returns = [[[]]*nA]*nS\n",
    "    # initialize pi arbitrarily\n",
    "    pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "    \n",
    "    #Loop forever (for max iter in order to ensure exit at some point)\n",
    "    for i in range(max_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        S_A_R = []\n",
    "        pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "        #Returns = [[[]]*nA]*nS\n",
    "        \n",
    "        # Generate an episode\n",
    "        while not done:\n",
    "            action = pi[state]\n",
    "            next_state, reward, done, __ = env.step(action)        \n",
    "            S_A_R.append([state,action,reward])\n",
    "            state = next_state\n",
    "        G = 0\n",
    "        \n",
    "        # for each step in episode, average\n",
    "        for i in range(len(S_A_R)-1,-1,-1):    \n",
    "            G = gamma*G + S_A_R[i][2]\n",
    "            \n",
    "            #Unless S_t and A_t appears in Episode until t\n",
    "            for S,A,R in S_A_R[:i-1]:\n",
    "                if (S,A,R) == S_A_R[i]:\n",
    "                    break\n",
    "            else:\n",
    "                #if G>0: print(G)\n",
    "                State,Action = S_A_R[i][0:2]\n",
    "                Returns[State][Action].append(G)\n",
    "                Q[State][Action] = np.mean(np.array(Returns[State][Action]))\n",
    "                \n",
    "                #Ensures policy doesn't pick 0 if max Q = 0, avoiding sub-optimal policies.\n",
    "                if np.any(Q[State]>0):\n",
    "                    pi[State] = np.argmax(Q[State])\n",
    "\n",
    "    return Q, pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Methods (TD)\n",
    "\n",
    "### TD(0)\n",
    "\n",
    "\n",
    "### SARSA\n",
    "### Q Learning\n",
    "    \n",
    "### Double Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_greedy(env, Q):\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        pi[s] = np.argmax(Q[s,:]) \n",
    "        #+ np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, lr=0.8, gamma=.95, max_episodes=2000):\n",
    "\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    Q = np.zeros([env.nS,env.nA])\n",
    "    #jList = []\n",
    "\n",
    "    rList = []\n",
    "    for i in range(max_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Table learning algorithm\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with noise) picking from Q table\n",
    "            a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            #Update Q-Table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                break\n",
    "        #jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    pi_Q = get_greedy(env, Q)\n",
    "    return Q, pi_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_PI, pi_PI = policy_iteration(env,300,0.99)\n",
    "V_VI, pi_VI = value_iteration(env,2000,0.99)\n",
    "\n",
    "\n",
    "# will take too many episodes because reward is super-sparse i.e given only at one state. \n",
    "#Prob of getting to this state by random actions is too low.\n",
    "Q_MC_ES, pi_MC_ES = monte_carlo_es(env,8000,.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_VI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_PI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_MC_ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 2, 1],\n",
       "       [0, 2, 2, 0],\n",
       "       [3, 2, 3, 2],\n",
       "       [3, 0, 3, 1]])"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_MC_ES.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 0, 3],\n",
       "       [0, 0, 0, 0],\n",
       "       [3, 1, 0, 0],\n",
       "       [0, 2, 1, 0]])"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_VI.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directions(pi,shape):\n",
    "    actions = ['left','down','right','up']\n",
    "    policy = np.array([actions[int(i)] for i in pi])\n",
    "    return policy.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play(env,pi_MC_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q = q_learning(env)\n",
    "#pi_QL = get_greedy(env,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_directions(pi_QL,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play(env,pi_QL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
