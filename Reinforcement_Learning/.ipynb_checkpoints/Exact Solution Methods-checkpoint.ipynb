{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-armed Bandits\n",
    "\n",
    "Choose from K different actions repeatedly and receive a numerical reward chosen from a stationary probability distribution conditional on the selected action each time independent of the previous action(s). \n",
    "\n",
    "In the K-armed Bandit problem, each action $a$ has an expected reward $q_*(a)$. This is called the value of that action.\n",
    "\n",
    "$  q_*(a) \\doteq \\mathbb E[R_t | A_t=a] $\n",
    "\n",
    "Knowing the value of each action would enable us to solve the problem by simply selecting the highest-valued action always. However, it is likely we will have only estimates of the action values at any time, $Q_t(a)$. \n",
    "The objective is to get $Q_t(a)$ as close to $q_t(a)$ as possible.\n",
    "\n",
    "### Action-value Methods\n",
    "\n",
    "One way to estimate $Q_t(a)$ is to average rewards over the timesteps\n",
    "\n",
    "$Q_t(a) \\doteq \\frac{\\sum^{t-1}_{i=1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum^{t-1}_{i=1}\\mathbb 1_{A_i=a}}$\n",
    "\n",
    "The best way to select an action, a.k.a the policy would be 'greedy':\n",
    "\n",
    "$A_t \\doteq \\underset{a}{argmax} Q_t(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Value Function\n",
    "\n",
    "$V^{*}_{k}(s) \\leftarrow \\underset{a}{max} \\underset{s'}{\\sum} P(s'|s,a)( R(s,a,s')  + \\gamma V^{*}_{k-1}(s')) $ \n",
    "\n",
    "k is timesteps from horizon\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "$V^{\\pi}_{k}(s) \\leftarrow \\underset{a}{\\sum}\\pi(a|s) \\underset{s'}{\\sum} P(s'|s,\\pi (s))(R(s,\\pi(s),s')+ \\gamma V^{\\pi}_{k-1}(s'))$\n",
    "\n",
    "## Policy Improvement\n",
    "\n",
    "$\\pi_{k+1}(s) \\leftarrow \\underset{a}{argmax}\\ q^{\\pi}(s,a)$\n",
    "\n",
    "Where $ q^{\\pi}(s,a) = \\underset{s'}{\\sum} P(s'|s,a)(R(s,\\pi(s),s')+ \\gamma V^{\\pi}(s'))$\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "1. Initialize Policy $\\pi$\n",
    "\n",
    "2. Evaluate: Until $V^{\\pi}_{k}(s) - V^{\\pi}_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , evalute $\\pi$ as $V^{\\pi}_{k}(s)$\n",
    "3. Improve: If $\\pi_{k}(s) \\ \\ != \\pi_{k+1}(s) <$ perform policy improvement using $V^{\\pi}_{k}(s)$\n",
    "\n",
    "## Value Iteration\n",
    "\n",
    "1. Initialize $V(s)$ for all s in $\\mathbb S $\n",
    "2. Until $V_{k}(s) - V_{k+1}(s) < $ tolerance: For all  $s$ in $\\mathbb S$ , $V_{k}(s)$ as\n",
    "\n",
    "$V_{k}(s) = \\underset{a}{max} \\underset{s}{\\sum} P(s'|s,a)(R(s,a,s')+ \\gamma V(s'))$\n",
    "3. Perform policy improvement using $V_{k}(s)$ such that $ \\pi \\approx \\pi *$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged(V,V_new, tol=10e-3):\n",
    "    \"\"\"\n",
    "    Checks whether first two arguments have converged to within the third argument, tolerance.\n",
    "    Arguments:\n",
    "        V : Numpy float array\n",
    "        V_new: Numpy float array\n",
    "        tol: float\n",
    "    Returns:\n",
    "        bool True or False\n",
    "    \"\"\"\n",
    "    if np.all(np.abs(V - V_new) < tol) : \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(pi, P, nS, nA, gamma=0.9, max_iter=1000, tol=10e-3):\n",
    "    V = np.zeros(nS)\n",
    "    V_new = V.copy()\n",
    "    for i in range(max_iter):\n",
    "        V = V_new.copy()\n",
    "        V_new = np.zeros(nS)\n",
    "        for state in range(nS):\n",
    "            for probability, next_state, reward, done in P[state][pi[state]]: \n",
    "                V_new[state] += probability*(reward + gamma*V[next_state])\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(pi, P, V, nS, nA, gamma=0.9):\n",
    "    pi_new = np.zeros(nS, dtype='int')\n",
    "    for state in range(nS):\n",
    "        B = np.zeros(nA)\n",
    "        q = -99\n",
    "        for action in range(nA):\n",
    "            for probability, next_state, reward, done in P[state][action]: \n",
    "                B[action] += probability*(reward + gamma*V[next_state])\n",
    "            if B[action]>q:\n",
    "                q = B[action]\n",
    "                pi_new[state] = action\n",
    "            elif B[action] == q:\n",
    "                if np.random.random() < 0.5:\n",
    "                    pi_new[state] == action       \n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, max_iter=1000, gamma=0.9, tol=10e-3):  \n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = np.zeros(nS, dtype=float)\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                q = 0\n",
    "                for probability, next_state, reward, done in P[state][action]: \n",
    "                    q += probability*(reward + gamma*V[next_state])\n",
    "                if V_new[state] < q:\n",
    "                    V_new[state] = q\n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        V = V_new.copy()\n",
    "    pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iter=20, gamma=0.9, tol=10e-3):\n",
    "    #nS number of States in env\n",
    "    nS = env.nS\n",
    "    #nA number of actions in env\n",
    "    nA = env.nA\n",
    "    # P is dynamics model of env as [state,action] dictionary with (probability, next_state, reward, done) \n",
    "    P = env.P\n",
    "    \n",
    "    V = np.zeros(nS,dtype=float)\n",
    "    pi = np.zeros(nS,dtype=int)\n",
    "    \n",
    "    pi = np.array([np.random.randint(nA) for i in range(nS)])\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        V_new = evaluate_policy(pi,P,nS,nA,gamma=gamma,tol=tol)\n",
    "        pi_new = improve_policy(pi,P,V_new,nS,nA,gamma=gamma)\n",
    "        \n",
    "        if converged(V,V_new,tol):\n",
    "            break\n",
    "        \n",
    "        V = V_new.copy()\n",
    "        pi=pi_new.copy()\n",
    "    \n",
    "    return V_new, pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_PI, pi_PI = policy_iteration(env,300,0.99)\n",
    "V_VI, pi_VI = value_iteration(env,20000,0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_VI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V_PI.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['left','down','right','up']\n",
    "policy = np.array([actions[int(i)] for i in pi_PI])\n",
    "#policy.reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, policy, max_episodes=10):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        state,reward,done,info = env.step(policy[state])\n",
    "        print(state,reward,done,info)\n",
    "        env.render()\n",
    "        if done:\n",
    "            env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#play(env,pi_VI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
