{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Sanity checks in functions |\n",
    "Regularisation |\n",
    "Prevent underflow/overflow |\n",
    "Refactor code to make more streamlined |\n",
    "Metrics: Precision, Recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_imgs(X):\n",
    "    n_img = np.shape(X)[0]\n",
    "    h = np.shape(X)[1]\n",
    "    w = np.shape(X)[2]\n",
    "    size_arr = h*w\n",
    "    return np.reshape(X,(n_img,size_arr))\n",
    "\n",
    "def one_hot(Y,n_class):\n",
    "    length = np.shape(Y)[1]\n",
    "    O = np.zeros((n_class,length))\n",
    "    for i in range(length):\n",
    "        j = int(Y[0,i])\n",
    "        O[j,i] = 1\n",
    "    return O\n",
    "\n",
    "def inv_one_hot(O):\n",
    "    n_class = np.shape(O)[0]\n",
    "    length = np.shape(O)[1]\n",
    "    Y = np.zeros((1,length))\n",
    "    for i in range(length):\n",
    "        j = np.argmax(O[:,i])\n",
    "        Y[0,i] = j\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X)\n",
    "    std =  np.std(X)\n",
    "    N_X = (X-mean)/(std)\n",
    "    return N_X\n",
    "\n",
    "def model_accuracy(H,Y):\n",
    "    n = np.shape(H)[1]\n",
    "    err = 0\n",
    "    for i in range(n):\n",
    "        if H[0,i]!=Y[0,i]:\n",
    "            err += 1\n",
    "    accuracy = (1 - err/n)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid:\n",
    "    def activate(Z):\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        dsig = np.multiply(self.activate(Z),(1-self.activate(Z)))\n",
    "        return dsig\n",
    "    \n",
    "class relu:\n",
    "    def activate(Z):\n",
    "        A = Z*(Z>0)\n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        d_rel = 1*(Z>0)\n",
    "        return d_rel\n",
    "    \n",
    "# wrong implementation of leaky\n",
    "class leaky_relu:\n",
    "    def activate(Z):\n",
    "        A = Z if (Z.all()>0.001*Z.all()) else 0.001*Z\n",
    "        return A\n",
    "    \n",
    "    def diff(self,Z):\n",
    "        d_lrel = 1 if (Z.all()>0.001*Z.all()) else 0.001\n",
    "        return d_lrel\n",
    "    \n",
    "class tanh:\n",
    "    def activate(Z):\n",
    "        A = np.tanh(Z)\n",
    "        return A\n",
    "\n",
    "    def diff(self,Z):\n",
    "        d_tanh = 1 - (np.multiply(self.activate(Z),self.activate(Z)))\n",
    "        return d_tanh\n",
    "    \n",
    "class softmax:\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    def activate(Z):\n",
    "        e_Z = np.exp(Z- np.max(Z,axis=0))\n",
    "        return e_Z / e_Z.sum(axis=0)\n",
    "    \n",
    "    def diff(Z):\n",
    "        return Z\n",
    "    \n",
    "class CE_loss:\n",
    "    def get_loss(H,Y):\n",
    "        L = -np.mean(np.multiply(Y,np.log(H)))\n",
    "        return L\n",
    "    \n",
    "    def diff(H,Y):\n",
    "        dZ = H - Y \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Matrix\n",
    "def init_theta(n1,n2,activation):\n",
    "    if activation in [sigmoid,softmax]:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(2./n1)\n",
    "    elif activation in [relu,leaky_relu] :\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./n1)\n",
    "    elif activation == tanh:\n",
    "        M = np.random.randn(n2,n1)*np.sqrt(1./(n1+n2))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = read_idx('data/MNIST/train/train-images-idx3-ubyte')\n",
    "X = flatten_imgs(X)\n",
    "X = normalize(X)\n",
    "X = np.transpose(X)\n",
    "\n",
    "Y = read_idx('data/MNIST/train/train-labels-idx1-ubyte')\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "Y = np.transpose(Y)\n",
    "Y = one_hot(Y,10)\n",
    "\n",
    "X_test = read_idx('data/MNIST/test/t10k-images.idx3-ubyte')\n",
    "X_test = flatten_imgs(X_test)\n",
    "X_test = normalize(X_test)\n",
    "X_test = np.transpose(X_test)\n",
    "\n",
    "Y_test = read_idx('data/MNIST/test/t10k-labels.idx1-ubyte')\n",
    "Y_test = np.expand_dims(Y_test, axis=1)\n",
    "Y_test = np.transpose(Y_test)\n",
    "Y_test = one_hot(Y_test,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABHdJREFUeJzt3TFOlF0UgGHnz4SEjsYEG21lCRTEWNGzC5fgJihZB8E90BFKaChoiIUmUBG0GLu/gov6wYzhfZ5yTsa5Cb45CTcfM1ssFq+Al++/VR8AWA6xQ4TYIULsECF2iJgv+fP86h+e3+y+F212iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUPEfNUH4Hn9/PlzOD8+Ph7OP3/+POn9/DtsdogQO0SIHSLEDhFihwixQ4TYIWK2WCyW+XlL/TBevfr27dtw/vr16+F8c3NzOD89PZ30fp7F7L4XbXaIEDtEiB0ixA4RYocIsUOER1wZ+vr166S5q7d/h80OEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0R4np1Jbm9vV30EfpPNDhFihwixQ4TYIULsECF2iBA7RLhnZ5KTk5PhfHt7e0kn4TE2O0SIHSLEDhFihwixQ4TYIULsEOGe/YWbz8c/4o2NjeH8+vp6OL+4uPjjM7EaNjtEiB0ixA4RYocIsUOE2CHC1dsL99jV2s7OznD+5cuXpzwOK2SzQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4Rnmdnku/fv6/6CPwmmx0ixA4RYocIsUOE2CFC7BAhdohwz84kR0dHqz4Cv8lmhwixQ4TYIULsECF2iBA7RIgdItyzx338+HE49/3sL4fNDhFihwixQ4TYIULsECF2iHD1Fvf27dtJ7//x48dwfnl5+eDs3bt3kz6bP2OzQ4TYIULsECF2iBA7RIgdIsQOEe7Z4+bzaf8FFovFcH53dzfp3+fp2OwQIXaIEDtEiB0ixA4RYocIsUPE7LF70ie21A9juq2treH8/Px8OP/06dODs4ODg786E4+a3feizQ4RYocIsUOE2CFC7BAhdogQO0R4np2h3d3d4fzq6mo439/ff8rjMIHNDhFihwixQ4TYIULsECF2iHD1xiSz2b1PU/5vbW1tSSfhMTY7RIgdIsQOEWKHCLFDhNghQuwQ4Z6dSW5ubobzw8PDB2d7e3tPfRwGbHaIEDtEiB0ixA4RYocIsUOE2CHCVzYz9ObNm+H8+vp6OD89PX1w9v79+786E4/ylc1QJnaIEDtEiB0ixA4RYocIsUOE59kZ+vDhw3B+dnY2nK+vrz/lcZjAZocIsUOE2CFC7BAhdogQO0SIHSI8zw4vj+fZoUzsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhHL/srme//ELfD8bHaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNgh4hdEKmfD6e9v1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 8\n",
    "plt.imshow(X[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(Y[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, n_prev, n_next, activation):\n",
    "        self.W = init_theta(n_prev, n_next, activation)\n",
    "        self.B = init_theta(1, n_next, activation)\n",
    "        self.activation = activation\n",
    "        self.V_dW = np.zeros(self.W.shape)\n",
    "        self.V_dB = np.zeros(self.B.shape)\n",
    "        \n",
    "    def forward(self, A0):\n",
    "        self.Z = np.dot(self.W, A0) + self.B\n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dZ, W, A0, m):\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        dAdZ = self.activation.diff(self.activation, self.Z)\n",
    "        self.dZ = np.multiply(dA, dAdZ)\n",
    "        self.dW = (1./m)*np.dot(self.dZ, A0.T)\n",
    "        self.dB = (1./m)*(np.sum(self.dZ, axis=1, keepdims=True))\n",
    "    \n",
    "    def out_grad(self, dZ, A0, m):\n",
    "        self.dZ = dZ\n",
    "        self.dW = (1./m)*np.dot(self.dZ, A0.T)\n",
    "        self.dB = (1./m)*(np.sum(self.dZ, axis=1, keepdims=True))\n",
    "        \n",
    "    def step(self, lr, beta):\n",
    "        self.V_dW = (beta * self.V_dW + (1. - beta) * self.dW)\n",
    "        self.V_dB = (beta * self.V_dB + (1. - beta) * self.dB)\n",
    "        self.W = self.W - lr*self.V_dW\n",
    "        self.B = self.B - lr*self.V_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_net:\n",
    "    def __init__(self, X_size, Y_size, lossfn):\n",
    "        self.L1 = layer(X_size, 500, relu)\n",
    "        self.L2 = layer(500, 150, relu)        \n",
    "        self.L3 = layer(150, Y_size, softmax)\n",
    "        self.lossfn = lossfn\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        A1 = self.L1.forward(X)\n",
    "        A2 = self.L2.forward(A1)\n",
    "        A3 = self.L3.forward(A2)\n",
    "        self.H = A3\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size):\n",
    "        m = batch_size\n",
    "        self.loss = self.lossfn.get_loss(self.H,Y)\n",
    "        dZ = self.lossfn.diff(self.H,Y)\n",
    "        self.L3.out_grad(dZ, self.L2.A, m)\n",
    "        self.L2.grad(self.L3.dZ, self.L3.W, self.L1.A, m)\n",
    "        self.L1.grad(self.L2.dZ, self.L2.W, X, m)\n",
    "    \n",
    "    def optim(self, lr, beta=0):\n",
    "        self.L1.step(lr,beta)\n",
    "        self.L2.step(lr,beta)\n",
    "        self.L3.step(lr,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(batch_size,X,Y,model,lr,beta):\n",
    "    m = np.shape(X)[1]\n",
    "    for i in range(0,m,batch_size):\n",
    "        X_batch = X[:,i:i+batch_size]\n",
    "        Y_batch = Y[:,i:i+batch_size]\n",
    "        model.f_pass(X_batch)\n",
    "        model.back_prop(X_batch,Y_batch,batch_size)\n",
    "        model.optim(lr,beta)\n",
    "    return model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model creation\n",
    "n_in = np.shape(X)[0]\n",
    "n_out = np.shape(Y)[0]\n",
    "mnist_net = MNIST_net(n_in,n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 32\n",
    "lr = 0.075\n",
    "n_epochs = 10\n",
    "lr_decay = 0.9\n",
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for e in range(n_epochs):\n",
    "    #shuffle dataset\n",
    "    np.random.seed(138)\n",
    "    shuffle_index = np.random.permutation(60000)\n",
    "    X, Y = X[:,shuffle_index], Y[:,shuffle_index]\n",
    "    \n",
    "    #SGD with momentum\n",
    "    loss = SGD(batch_size,X,Y,mnist_net,lr,beta)\n",
    "    \n",
    "    lr = lr*lr_decay\n",
    "    \n",
    "    H = mnist_net.f_pass(X)\n",
    "    O = inv_one_hot(H)\n",
    "    L = inv_one_hot(Y)\n",
    "    tr_acc = model_accuracy(O,L)\n",
    "    \n",
    "    H = mnist_net.f_pass(X_test)\n",
    "    O = inv_one_hot(H)\n",
    "    L = inv_one_hot(Y_test)\n",
    "    acc = model_accuracy(O,L)\n",
    "    \n",
    "    plt.plot(e,tr_acc, 'bo')\n",
    "    plt.plot(e,acc,'ro')\n",
    "    clear_output()\n",
    "    print(f\"epoch:{e+1}/{n_epochs} | Loss:{loss:.4f} | Train Accuracy: {tr_acc:.4f} | Test_Accuracy:{acc:.4f}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlinepub",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
