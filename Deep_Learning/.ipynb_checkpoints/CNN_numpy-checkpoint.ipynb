{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atlas_ml import *\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(img_matrix, padding):\n",
    "    pad_width = [(0,0),(0,0),(padding[0],padding[0]),(padding[1],padding[1])]\n",
    "    padded_matrix = np.pad(img_matrix, pad_width=pad_width, mode='constant',)\n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing zero_padding()\n",
    "A = np.arange(2*3*4*4).reshape(2,3,4,4)\n",
    "P = zero_padding(A,[2,2])\n",
    "#print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/30109068/implement-matlabs-im2col-sliding-in-python\n",
    "def im2col(imgs, kernel_size, stride):\n",
    "    # Parameters\n",
    "    F = kernel_size\n",
    "    batch_size, D,H,W = imgs.shape\n",
    "    col_extent = (W - F[1]) + 1\n",
    "    row_extent = (H - F[0]) + 1\n",
    "\n",
    "    # Get batch block indices\n",
    "    batch_idx = np.arange(batch_size)[:, None, None] * D * H * W\n",
    "    # Get Starting block indices\n",
    "    start_idx = np.arange(F[0])[None, :,None]*W + np.arange(F[1])\n",
    "    # Generate Depth indices\n",
    "    didx=H*W*np.arange(D)\n",
    "    start_idx=(didx[None, :, None]+start_idx.ravel()).reshape((-1,F[0],F[1]))\n",
    "\n",
    "    # Get offsetted indices across the height and width of input array\n",
    "    offset_idx = np.arange(row_extent)[None, :, None]*W + np.arange(col_extent)\n",
    "    \n",
    "    # Get all actual indices & index into input array for final output\n",
    "    act_idx = (batch_idx + \n",
    "        start_idx.ravel()[None, :, None] + \n",
    "        offset_idx[:,::stride[0],::stride[1]].ravel())\n",
    "\n",
    "    col_matrix = np.take (imgs, act_idx)\n",
    "    return col_matrix, act_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Testing im2col()\n",
    "A = np.random.rand(5,3,28,28) # 3 Sample input array with 1 channel\n",
    "kernel_size = [3,3] # Sample blocksize (rows x columns)\n",
    "stride = [1,1]\n",
    "out, act_idx = im2col(A,kernel_size,stride)\n",
    "#print(A.shape)\n",
    "#print(out.shape)\n",
    "#print(act_idx.shape)\n",
    "dA1 = np.random.rand(*out.shape)\n",
    "dA2 = np.zeros(A.shape)\n",
    "#print(dA1)\n",
    "for i in range(dA2.shape[-1]):\n",
    "    dA2.ravel()[act_idx[:,:,i].ravel()] += dA1[:,:,i].ravel()\n",
    "#print(dA2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filters(K, D, kernel_size, activation):\n",
    "        W = init_matrix(D*kernel_size[0]*kernel_size[1],K,activation)\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, n_prev, n_next, activation):\n",
    "        self.W = init_matrix(n_prev, n_next, activation)\n",
    "        self.B = init_matrix(1, n_next, activation)\n",
    "        self.activation = activation()\n",
    "        \n",
    "        self.V_dW = np.zeros(self.W.shape)\n",
    "        self.V_dB = np.zeros(self.B.shape)\n",
    "        \n",
    "    def forward(self, A0):\n",
    "        self.Z = np.einsum('ln,ml-> mn',self.W, A0) + self.B\n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dA, A0, m):\n",
    "        dAdZ = self.activation.diff(self.Z)\n",
    "        self.dZ = np.multiply(dA, dAdZ)\n",
    "        self.dW = (1./m)*np.einsum('mn,ml->ln',self.dZ, A0)\n",
    "        self.dB = (1./m)*(np.einsum('mn->n',self.dZ))\n",
    "        dA_prev = np.einsum('ln,mn->ml',self.W, self.dZ) \n",
    "        return dA_prev\n",
    "    \n",
    "    def out_grad(self, dZ, A0, m):\n",
    "        self.dZ = dZ\n",
    "        self.dW = (1./m)*np.einsum('mn,ml->ln',self.dZ, A0)\n",
    "        self.dB = (1./m)*(np.einsum('mn->n',self.dZ))\n",
    "        dA_prev = np.einsum('ln,mn->ml',self.W, self.dZ) \n",
    "        return dA_prev\n",
    "        \n",
    "    def step(self, lr, beta):\n",
    "        self.V_dW = (beta * self.V_dW + (1. - beta) * self.dW)\n",
    "        self.V_dB = (beta * self.V_dB + (1. - beta) * self.dB)\n",
    "        self.W = self.W - lr*self.V_dW\n",
    "        self.B = self.B - lr*self.V_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_layer:\n",
    "    def __init__(self, kernel_size, n_channels, activation, n_filters = 1, stride =[1,1],  padding = [0,0]):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_filters = n_filters\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation = activation()\n",
    "        \n",
    "        #intialize [n_filters,n_channels,kernel_size[0],kernel_size[1]] filters\n",
    "        self.W = init_filters(self.n_filters, self.n_channels, self.kernel_size, activation)\n",
    "        self.B = init_matrix( 1, n_filters, activation)\n",
    "        \n",
    "        self.V_dW = np.zeros(self.W.shape)\n",
    "        self.V_dB = np.zeros(self.B.shape)\n",
    "        \n",
    "    def get_output_shape(self, A_prev_shape):\n",
    "        batch_size,D,W,H = A_prev_shape\n",
    "        H_out = (H - self.kernel_size[0])//self.stride[0] + 1\n",
    "        W_out = (W - self.kernel_size[1])//self.stride[1] + 1\n",
    "        output_shape = (batch_size, self.n_filters, H_out, W_out)\n",
    "        return output_shape\n",
    "    \n",
    "    def forward(self, A_prev):\n",
    "        A_prev = zero_padding(A_prev, self.padding)        \n",
    "        self.A_prev_shape = A_prev.shape\n",
    "        self.im2cols, self.act_idx = im2col(A_prev,self.kernel_size, self.stride)\n",
    "        output_shape = self.get_output_shape(A_prev.shape)\n",
    "        \n",
    "        self.Z = np.einsum('mfn,fk->mnk',self.im2cols,self.W) + self.B   \n",
    "        self.A = self.activation.activate(self.Z)\n",
    "        self.A = np.transpose(self.A,(0,2,1)).reshape(output_shape)\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dA):\n",
    "        batch_size = dA.shape[0]\n",
    "        dAdZ = self.activation.diff(self.Z)\n",
    "        self.dZ = np.einsum('mjk,mjk->mjk', dA, dAdZ)\n",
    "        \n",
    "        self.dW = (1./batch_size)*np.einsum('mjk,mkl->jl', self.im2cols, self.dZ)\n",
    "        self.dB = (1./batch_size)*np.einsum('mjk->k', self.dZ)\n",
    "        \n",
    "        dA_prev_cols = np.einsum('mnk,fk-> mfn',self.dZ, self.W) \n",
    "\n",
    "        dA_prev = np.zeros(self.A_prev_shape)        \n",
    "        for i in range(self.act_idx.shape[-1]):\n",
    "            dA_prev.ravel()[self.act_idx[:,:,i].ravel()] += dA_prev_cols[:,:,i].ravel()\n",
    "\n",
    "        dA_prev = dA_prev.reshape(dA_prev.shape[0],dA_prev.shape[1],dA_prev.shape[2]*dA_prev.shape[3])\n",
    "        dA_prev = np.transpose(dA_prev,(0,2,1))\n",
    "        \n",
    "        return dA_prev\n",
    "    \n",
    "    def step(self, lr, beta, reg_lambda=0):\n",
    "        self.V_dW = (beta * self.V_dW + (1. - beta) * self.dW)\n",
    "        self.V_dB = (beta * self.V_dB + (1. - beta) * self.dB)\n",
    "        self.W    =  self.W - lr*self.V_dW\n",
    "        self.B    =  self.B - lr*self.V_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_layer():\n",
    "    def __init__(self, kernel_size, n_channels, stride =[1,1],  padding = [0,0]):\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_channels = n_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def get_output_shape(self, A_prev_shape):\n",
    "        batch_size,D,W,H = A_prev_shape\n",
    "        H_out = (H - self.kernel_size[0])//self.stride[0] + 1\n",
    "        W_out = (W - self.kernel_size[1])//self.stride[1] + 1\n",
    "        output_shape = (batch_size, self.n_channels, H_out, W_out)\n",
    "        return output_shape\n",
    "    \n",
    "    def forward(self, A_prev):\n",
    "\n",
    "        A_prev = zero_padding(A_prev, self.padding)\n",
    "        self.A_prev_shape = A_prev.shape\n",
    "        self.im2cols, self.act_idx = im2col(A_prev, self.kernel_size, self.stride)\n",
    "        \n",
    "        output_shape = self.get_output_shape(A_prev.shape)\n",
    "        \n",
    "        self.Z = np.split(self.im2cols,self.n_channels,axis=1)\n",
    "        self.Z = np.stack(self.Z, axis=1)\n",
    "\n",
    "        self.A = np.max(self.Z,axis=2)  \n",
    "        self.arg = np.argmax(self.A,axis=2)\n",
    "        \n",
    "        self.A = self.A.reshape(output_shape)\n",
    "        return self.A\n",
    "    \n",
    "    def grad(self, dA):\n",
    "        batch_size = dA.shape[0]\n",
    "        dA = np.expand_dims(dA.transpose(0,2,1), axis=2)\n",
    "        A = self.A.reshape(self.A.shape[0],self.A.shape[1],1,self.A.shape[-2]*self.A.shape[-1])\n",
    "        dAdZ = (self.Z - A)==0\n",
    "        self.dZ = np.multiply(dA, dAdZ) \n",
    "        shape = self.dZ.shape\n",
    "        self.dZ = self.dZ.reshape(shape[0],shape[1]*shape[2],shape[3])\n",
    "        \n",
    "        dA_prev = np.zeros(self.A_prev_shape)        \n",
    "        \n",
    "        for i in range(self.act_idx.shape[-1]):\n",
    "            dA_prev.ravel()[self.act_idx[:,:,i].ravel()] += self.dZ[:,:,i].ravel()\n",
    "\n",
    "        dA_prev = dA_prev.reshape(dA_prev.shape[0],dA_prev.shape[1],dA_prev.shape[2]*dA_prev.shape[3])\n",
    "        dA_prev = np.transpose(dA_prev,(0,2,1))\n",
    "        \n",
    "        return dA_prev\n",
    "    \n",
    "    def step(self, lr, beta, reg_lambda=0):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN:\n",
    "    def __init__(self, Y_size, lossfn, n_channels=1):\n",
    "        self.L1 = conv_layer([5,5], n_channels, leaky_relu, n_filters = 10,padding = [2,2])\n",
    "        self.L2 = conv_layer([3,3], 10, leaky_relu, n_filters = 15)\n",
    "        self.L3 = max_pool_layer([2,2],15, stride=[2,2])\n",
    "        self.L4 = conv_layer([3,3], 15, leaky_relu)        \n",
    "        self.L5 = layer(121,50, leaky_relu)\n",
    "        self.L6 = layer(50, Y_size, softmax)\n",
    "        self.lossfn = lossfn()\n",
    "        \n",
    "    def f_pass(self, X):\n",
    "        A = self.L1.forward(X)\n",
    "        A = self.L2.forward(A)\n",
    "        A = self.L3.forward(A)\n",
    "        A = self.L4.forward(A)\n",
    "        A.resize(A.shape[0], A.shape[2]*A.shape[3])\n",
    "        A = self.L5.forward(A)\n",
    "        A = self.L6.forward(A)\n",
    "        self.H = A\n",
    "        return self.H\n",
    "    \n",
    "    def back_prop(self,X,Y, batch_size,reg_lambda=0):\n",
    "        m = batch_size\n",
    "        self.loss = self.lossfn.get_loss(self.H,Y)\n",
    "        dZ = self.lossfn.diff(self.H,Y)\n",
    "        dA = self.L6.out_grad(dZ, self.L5.A, m)\n",
    "        dA = self.L5.grad(dA,self.L4.A, m)\n",
    "        dA = np.expand_dims(dA,axis=-1)\n",
    "        dA = self.L4.grad(dA)\n",
    "        dA = self.L3.grad(dA)\n",
    "        dA = self.L2.grad(dA)\n",
    "        dX = self.L1.grad(dA)\n",
    "    \n",
    "    def optim(self, lr, beta=0):\n",
    "        self.L1.step(lr,beta)\n",
    "        self.L2.step(lr,beta)\n",
    "        self.L3.step(lr,beta)\n",
    "        self.L4.step(lr,beta)\n",
    "        self.L5.step(lr,beta)\n",
    "        self.L6.step(lr,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_path = 'MNIST_np/data/MNIST/train/train-images-idx3-ubyte'\n",
    "trainY_path = 'MNIST_np/data/MNIST/train/train-labels-idx1-ubyte'\n",
    "testX_path  = 'MNIST_np/data/MNIST/test/t10k-images.idx3-ubyte'\n",
    "testY_path  = 'MNIST_np/data/MNIST/test/t10k-labels.idx1-ubyte'\n",
    "\n",
    "X,Y,X_test,Y_test = load_mnist_data(trainX_path,trainY_path,testX_path,testY_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = np.shape(Y)[1]\n",
    "mnist_cnn = MNIST_CNN(n_out,CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "lr = 0.005\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "lr_decay = 0.9\n",
    "\n",
    "data_size = X.shape[0]\n",
    "\n",
    "beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10/10 | Loss:0.0992 Train Accuracy: 0.9296 | Test_Accuracy:0.9290\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(mnist_cnn, X, Y, X_test, Y_test, model_accuracy, n_epochs, \\\n",
    "    batch_size, lr, lr_decay, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure time taken\n",
    "start = time. time()\n",
    "mnist_cnn.f_pass(X[0:4])\n",
    "endf = time. time()\n",
    "mnist_cnn.back_prop(X[0:4],Y[0:4],4)\n",
    "endb = time. time()\n",
    "mnist_cnn.optim(lr,beta)\n",
    "endo = time. time()\n",
    "mnist_cnn.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"f-pass:{endf-start:.4f} | b-prop:{endb-endf:.4f} | optim: {endo-endb:.4f} | total: {endo-start:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Does not work currently\n",
    "\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_model(filename):\n",
    "    model = None\n",
    "    with open(filename, 'r') as f:  # Overwrites any existing file.\n",
    "        model = pickle.load(f)   \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlinepub",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
